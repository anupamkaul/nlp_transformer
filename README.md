.
├── README.md
├── topic1
│   ├── links.txt
│   └── papers
│       ├── AttentionTransformer.pdf
│       ├── SemiSupervisedSeqLearning.pdf
│       └── start_language_understanding_paper.pdf
└── transformers
    └── notebooks
        ├── 01_introduction.ipynb
        ├── 02_classification.ipynb
        ├── 03_transformer-anatomy.ipynb
        ├── 04_multilingual-ner.ipynb
        ├── 05_text-generation.ipynb
        ├── 06_summarization.ipynb
        ├── 07_question-answering.ipynb
        ├── 07_question_answering_v2.ipynb
        ├── 08_model-compression.ipynb
        ├── 09_few-to-no-labels.ipynb
        ├── 10_transformers-from-scratch.ipynb
        ├── 11_future-directions.ipynb
        ├── LICENSE
        ├── README.md
        ├── SageMaker
        │   ├── 01_introduction.ipynb
        │   ├── 02_classification.ipynb
        │   ├── README.md
        │   ├── images
        │   │   ├── git_repo.png
        │   │   ├── iam_role.png
        │   │   └── notebook_config.png
        │   ├── scripts
        │   │   └── 02_classification_train.py
        │   └── utils.py
        ├── data
        │   └── github-issues-transformers.jsonl
        ├── environment-chapter7.yml
        ├── environment.yml
        ├── images
        │   ├── book_cover.jpg
        │   ├── chapter01_enc-dec-attn.png
        │   ├── chapter01_enc-dec.png
        │   ├── chapter01_hf-ecosystem.png
        │   ├── chapter01_hub-model-card.png
        │   ├── chapter01_hub-overview.png
        │   ├── chapter01_rnn.png
        │   ├── chapter01_self-attention.png
        │   ├── chapter01_timeline.png
        │   ├── chapter01_transfer-learning.png
        │   ├── chapter01_ulmfit.png
        │   ├── chapter02_attention-alignment.png
        │   ├── chapter02_attention-mask.png
        │   ├── chapter02_encoder-classifier.png
        │   ├── chapter02_encoder-feature-based.png
        │   ├── chapter02_encoder-fine-tuning.png
        │   ├── chapter02_hf-libraries.png
        │   ├── chapter02_transformers-compact.html
        │   ├── chapter02_transformers.html
        │   ├── chapter02_transformers.png
        │   ├── chapter02_tweet.png
        │   ├── chapter03_attention-ops.png
        │   ├── chapter03_contextualized-embedding.png
        │   ├── chapter03_decoder-zoom.png
        │   ├── chapter03_encoder-zoom.png
        │   ├── chapter03_layer-norm.png
        │   ├── chapter03_multihead-attention.png
        │   ├── chapter03_transformer-encoder-decoder.png
        │   ├── chapter03_transformers-compact.png
        │   ├── chapter04_bert-body-head.png
        │   ├── chapter04_clf-architecture.png
        │   ├── chapter04_ner-architecture.png
        │   ├── chapter04_ner-widget.png
        │   ├── chapter04_tokenizer-pipeline.png
        │   ├── chapter05_beam-search.png
        │   ├── chapter05_lm-meta-learning.png
        │   ├── chapter05_meena.png
        │   ├── chapter05_text-generation.png
        │   ├── chapter07_dpr.png
        │   ├── chapter07_marie-curie.png
        │   ├── chapter07_phone.png
        │   ├── chapter07_qa-head.png
        │   ├── chapter07_qa-pyramid.png
        │   ├── chapter07_rag-architecture.png
        │   ├── chapter07_retriever-reader.png
        │   ├── chapter07_sliding-window.png
        │   ├── chapter07_squad-models.png
        │   ├── chapter07_squad-schema.png
        │   ├── chapter07_squad-sota.png
        │   ├── chapter08_bert-onnx.png
        │   ├── chapter08_fp32-to-int8.png
        │   ├── chapter08_kd.png
        │   ├── chapter08_magnitude-vs-movement.png
        │   ├── chapter08_network-pruning.png
        │   ├── chapter08_onnx-ort.png
        │   ├── chapter08_oos.png
        │   ├── chapter08_pegasus.png
        │   ├── chapter08_pruning-dists.png
        │   ├── chapter08_roblox.png
        │   ├── chapter08_soft-probs.png
        │   ├── chapter08_t5.png
        │   ├── chapter09_decision-tree.png
        │   ├── chapter09_faiss-index.png
        │   ├── chapter09_issue-example-v2.png
        │   ├── chapter09_nearest-neighbours.png
        │   ├── chapter09_uda.png
        │   ├── chapter09_ust.png
        │   ├── chapter10_code-snippet.png
        │   ├── chapter10_ddp.png
        │   ├── chapter10_preprocessing-clm.png
        │   ├── chapter10_pretraining-clm.png
        │   ├── chapter10_pretraining-mlm.png
        │   ├── chapter10_pretraining-seq2seq.png
        │   ├── chapter11_atomic-sparse-attention.png
        │   ├── chapter11_clip-arch.png
        │   ├── chapter11_compound-sparse-attention.png
        │   ├── chapter11_dall-e.png
        │   ├── chapter11_efficient-attention.png
        │   ├── chapter11_iGPT.png
        │   ├── chapter11_layoutlm.png
        │   ├── chapter11_linear-attention.png
        │   ├── chapter11_scaling-modal.png
        │   ├── chapter11_scaling.png
        │   ├── chapter11_table-qa.png
        │   ├── chapter11_tapas-architecture.png
        │   ├── chapter11_vit-architecture.png
        │   ├── chapter11_vqa.png
        │   ├── chapter11_wav2vec-u.png
        │   ├── chapter11_wav2vec2.png
        │   ├── doge.jpg
        │   └── optimusprime.jpg
        ├── install.py
        ├── plotting.mplstyle
        ├── requirements-chapter7-v2.txt
        ├── requirements-chapter7.txt
        ├── requirements.txt
        ├── scripts
        │   └── create_notebook_table.py
        ├── settings.ini
        └── utils.py

11 directories, 129 files
